\section{Models}

In this section all models are discussed which are used or created in this work.
This section is divided into three subsections.
Each of them corresponds to one of the main contributions of this work.
Therefore the first section gives a brief overview of used object detection models, the second one with the \ac{TEP}-Net \cite{tepNet2024} because it is used as a baseline.
Additionally, further improvements of this model are described in this section.
The last section describes all temporal models.

\subsection{Object Detection Models}

The state of the art in \autoref{sec:ObjectDetection} shows that one-stage detectors have the most promising characteristics to be successfully utilized in this work.
The most interesting ones are the models from the \ac{YOLO} series, especially the most recent ones because they focus on improved parameter usage.
This makes the models light weight and presents an advantage for this work, because operation on limited hardware is aimed for.
At the start of this work the most recent model from the \ac{YOLO} series is the \ac{YOLO}v9 \cite{YOLOv9}.
For object detection different models of the \ac{YOLO}v9 and the \ac{GELAN} series are available.
However, the \ac{YOLO}v9 models were not fully supported at that time.
Therefore the models used for experiments include the \ac{GELAN}-c and \ac{GELAN}-e.
These are obtained trough the GitHub repository \cite{YOLOv9GitHub} and used unchanged.
An additional model which is used for experiments is the \ac{YOLO}v7 \cite{yolov7} model, which is also utilized unchanged from the GitHub repository \cite{YOLOv7GitHub}.

\subsection{TEP-Net Model}
\label{subsec:baselineModel}

In the literature rails are often detected without distinction between all rails visible in an image and the rail the train continues.
\cite{tepNet2024} therefore proposes a regression-based approach, which restricts the model to predict a single track.
The idea comes from various lane detection methods in autonomous driving applications for road cars and is fitted to the rail domain.
Since, this method fits well to the goal of this work, it is used as a baseline for further experiments.

Although rails can be represented by second or third-degree polynomials most of the time \cite{PolyLaneNetRoad2021}, they may also take more complex geometric shapes.
Hence, limiting the output to presumed forms is discouraged.
Therefore, the method used in \cite{tepNet2024} employs spline interpolation to describe such complex structures.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{PICs/Baselinepaper/TEP-Net_model.jpg}
    \caption{\ac{TEP}-Net model architecture\cite{tepNet2024}. The input of the model is a cropped and resized image and the output of the model are the $x$-values for the left and right rail on each anchor line plus an additional value for the $y$-limit.}
    \label{fig:TEP-Net_model}
\end{figure}

As shown in the prediction of \autoref{fig:TEP-Net_model}, a set of horizontal lines are overlayed in the cropped image.
The number of $y$-lines or "anchors" is determined by a hyperparameter.
They are uniformly distributed along the $y$-axis. For each line, two $x$-values are predicted.
One for the left rail and one for the right rail.
The prediction image of \autoref{fig:TEP-Net_model} and the second and fourth images in the bottom row of \autoref{fig:tepNet_dataaugmentation} show that rails do not necessarily cross with anchor lines at the top of image crops.
Therefore, an additional $y$-limit is predicted, which gives information up to which anchor the rail should be detected.
Anchors and their $x$-values above this horizon line do not hold valuable information and are ignored.

For this novel regression task, a new model architecture is created.
For this model widely used backbone architectures like ResNet and EfficientNet are chosen.
The input of the backbones is an image crop with the size of $3 \times 512 \times 512$ which represents a rough average of crops obtained by data augmentation strategies.
These backbones extract relevant features from these crops reducing the spatial dimensions and increasing channel size to a high number.
After that, the feature map's number of channels is reduced to a predefined size with a 1x1 Conv2d layer \cite{pytorch_conv2d_docu} and flattened to a vector.
This feature vector serves as the input for the prediction head, which consists of two fully connected layers \cite{pytorch_linearLayer_docu} in series.
The size of the linear layers is set with a hyperparameter.
In the last layer, a reduction leads to the resulting prediction vector with the dimension $2 \times H + 1$.
This vector includes the entire information of one prediction.
$H$ is the number of anchors.
The first set of values with size $H$ is for the left rail, another $H$ set for the right rail and the $+ 1$ is for the last values being the horizon line.

This architecture's policy for value ranges does not restrict the $x$-values in any way.
This way predictions can be outside of a crop and the model can learn that sometimes the rails extend out of the viewed field.
The $y$-limit on the other hand is constrained to a range between 0 and 1 with a Sigmoid function.

The introduced model architecture can be classified as an end-to-end framework.
This means the model can be trained and used for inference without any steps in between.
It takes in raw data and results in a complete prediction.


\subsection{Improvements to the TEP-Net Model}
\label{subsec:improvemensts2BaselineModel}

One of the main contributions of this work is to improve the chosen baseline model from \cite{tepNet2024}.
These improvements can be roughly divided into three parts.
First, the backbones are exchanged with other architectures.
Secondly, after the backbones, a true pooling layer is integrated.
Thirdly, experiments with different prediction heads are conducted.
The following sections describe the structure of these architecture changes in detail.

\subsubsection{Backbones}

Backbones are used for extracting features from images.
These are parts of \ac{CNN}s that input and output tensors.
Common \ac{CNN} architectures usually work with 4D tensors with $B \times C \times H \times W$ as shown in \autoref{fig:backboneLogic}.
$B$ is the batch size, $C$ is the channel size, $H$ and $W$ are the height and width of the input image.
The backbone transforms this high-level feature map into a low-level one with different operations, resulting in a tensor with a high number of channels $C$ and low numbers in resolution $H$ and $W$.
The batch size $B$ tells how many images the \ac{CNN} processes simultaneously and stays the same.
These characteristics apply to all four chosen backbones.
As described in the state of the art in \autoref{sec:networkArchitectures}, the most interesting models are ResNet \cite{ResNet}, EfficientNet \cite{EfficientNet}, DenseNet \cite{DenseNets}, and MobileNetV3 \cite{MobileNetV3}.
ResNet and EfficientNet are already implemented in \cite{tepNet2024}.
DenseNet and MobileNet are additionally integrated in this work.
\autoref{tab:backboneVersions} gives an overview of all different versions which are implemented in this work.

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|}
    %\begin{tabular}{| p{0.3\linewidth} | p{0.6\linewidth} |}
        \hline
        ResNet & 18, 34, 50\\
        \hline
        MobileNetV3 & small, large\\
        \hline
        DenseNet & 121, 161, 169, 201\\
        \hline
        EfficientNet & B0, B1, B2, B3\\
        \hline
    \end{tabular}
    \caption{Versions of backbones utilized in this work}
    \label{tab:backboneVersions}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{PICs/improvedModel/backbone_logic.jpg}
    \caption{Backbone logic (backbone graphic taken from\cite{tepNet2024}).}
    \label{fig:backboneLogic}
\end{figure}

When exchanging backbone architectures it is important, that the dimensions of input and output tensors are the same for every backbone otherwise the next layers would get different input dimensions than they are designed for. Standardizing the input tensor is done by resizing an image to 512x512 after augmentation. While B is set by a hyperparameter, C = 3 because images are in the 3-channel RGB format. For the output dimensions, the backbone architectures and their operations must be analyzed. When inspecting all four backbones in more detail it becomes clear that they all share the following characteristics:

\begin{itemize}
    \item The input resolution in all papers is 224x224.
    \item At the end, there is either a pooling layer, a 1x1 convolutional layer, or both in combination with fully connected layers forming a prediction head for classification tasks.
    \item Before the prediction head, the spatial resolution of feature maps is 7x7.
\end{itemize}

With this information, the last layers of each backbone can be discarded, so all backbones output tensors with the same $H$ and $W$.
In all four papers, the architecture transforms tensors with a resolution of $224 \times 224$ to a tensor with $7 \times 7$ before the prediction heads.
This means they all have the same reduction factor of 32.
Since the input resolution of $512 \times 512$ is used in this work resulting feature maps have a resolution of $16 \times 16$ after backbones.
The exact dimensions of all tensors are shown in \autoref{tab:backboneDimensions}.

\begin{table}[H]
\centering
\begin{tabular}{|r|p{0.2\linewidth}|r@{ }r@{,  }r@{,  }r@{,  }r@{ }l|}\hline
%\begin{tabular}{| p{0.3\linewidth} | p{0.3\linewidth} | p{0.3\linewidth} |}\hline
\multicolumn{1}{|l|}{\textbf{Backbone}}       & \textbf{Input dimensions} & \multicolumn{6}{|l|}{\textbf{Output dimensions}}\\\hline
ResNet                  & \multirow{4}{=}{\centering [ 8, 3, 512, 512 ]}  & [ & 8 &  512 & 16 & 16 &]\\\cline{1-1} \cline{3-8}
MobileNetV3             &                                               & [ & 8 &   96 & 16 & 16 &]\\\cline{1-1} \cline{3-8}
DenseNet                &                                               & [ & 8 & 1024 & 16 & 16 &]\\\cline{1-1} \cline{3-8}
EfficientNet            &                                               & [ & 8 &  384 & 16 & 16 &]\\\hline
\end{tabular}
\caption{Versions of backbones utilized in this work}
\label{tab:backboneDimensions}
\end{table}

\subsubsection{Pooling Layer}

blindtext

\subsubsection{prediction Heads}

blindtext


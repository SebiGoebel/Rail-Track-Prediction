\section{Network architectures}
\label{sec:networkArchitectures}

One of the first CNN architectures was the \textbf{Le-Net5} \cite{LeNet5}.
This CNN was proposed to classify handwritten digits.
It used 5 convolution and pooling layers.
14 years later CNNs were rediscovered with the introduction of \textbf{AlexNet} \cite{AlexNet2012} in 2012.
In the \ac{ILSVRC} \cite{ILSVRC15} in 2012 \textbf{AlexNet} outperformed traditional algorithms and showed the potential of CNNs.
Consequently, it is often considered the starting point of the CNN era in the literature.
Since then research has shown a rapid development of CNN architectures, consistently striving for greater accuracy or speed \cite{networkArchitectureSurvey}.

\vspace{1cm} % Größerer Abstand zwischen den Reihen

\noindent Several major advancements up to now include:

\begin{itemize}
    \item In 2014 two popular architectures were introduced, which still are often used for state-of-the-art comparisons. Firstly, the \textbf{\ac{VGGNet}} \cite{VGGNet2015} used small three-by-three filters and many layers providing a deep network architecture. Secondly, the \textbf{Inception} network or \textbf{GoogLeNet} \cite{InceptionNet}. To extract features in a range of various scales, this architecture utilized different filter sizes.
    \item In 2015 the \textbf{ResNet} architecture \cite{ResNet} introduced the concept of residual learning with shortcut connections. By skipping layers, this framework enabled the training of deeper networks with over a hundred layers.
    \item In 2016 \textbf{DenseNets} \cite{DenseNets} was introduced to further deal with the problem of vanishing gradients. This architecture implemented a connection between each layer and every following layer ensuring maximum information flow.
    \item In 2017 \textbf{MobileNets} \cite{MobileNetV1} \cite{MobileNetV2} \cite{MobileNetV3} was developed for applications in mobile devices. Since hardware with limited capabilities is used, the main focus of MobileNet architectures lies on efficiency. The utilization of depthwise separable convolutions allowed small model sizes and fast latencies.
    \item In 2019 \textbf{EfficientNets} \cite{EfficientNet} was introduced to further explore greater trade-offs between accuracy and efficiency, by using scaling methods for model architectures. 
\end{itemize}

\vspace{1cm} % Größerer Abstand zwischen den Reihen

\noindent Since the CNNs are a part of a rapidly developing research field, more recent architectures are more interesting for this work.
Because they already solved issues that exist in older models.
Therefore, only the last four mentioned CNNs are described in more detail in the next sections.

\subsection{ResNets}

https://medium.com/@cpt1995daas/evolution-of-convolutional-neural-network-cnn-architectures-44f2109268a1

A widely used model architecture in the community is ResNet <Quelle: ResNet>.
Up to this architecture CNNs suffered from the vanishing gradient problem, particularly when the model consists of a deep network.
A neural network is typically considered deep when it comprises a large number of consecutive layers.
To solve that issue ResNet introduced the residual block that is also visualized in <Figure xy>.
This block enabled effective training of models with up to 152 layers. Compared to VGGNet ResNet is 8 times deeper but still less complex.
ResNet showed a great performance gain and was therefore granted first place in the classification task of the ILSVRC 2015.
ResNet is supported in Pytorch, which includes variants with 18, 34, 50, 101, and 152 layers <Quelle: https://pytorch.org/vision/main/models/resnet.html>. 
However, ResNet models are still resource-intensive because of their size <Quelle: Übersicht>. 

\vspace{1cm}

<Figure xy> <Quelle: ResNet>

\vspace{1cm}

<Figure xy> schematically represents the ResNet's skip connection.
The block has two processes.
One consists of two layers and the other one performs identity mapping.
After that, the outputs are added together.

\subsection{DenseNets}

DenseNet further explores skip connections and implemented architecture blocks in which each layer is connected to every following layer.
An example of a so-called "dense block" is shown in <Figure xy>.
Compared to ResNet, this model connects outputs via concatenation instead of addition.
Between dense bocks, there are the so-called "transition layers", consisting of one convolutional and then one pooling layer.
This architecture allows deeper CNNs which result in more accuracy and efficiency.
DenseNet showed great results with the introduced technique because their blocks further tackle the vanishing gradient problem with feature reuse.
PyTorch includes DenseNet model variants with 121, 161, 169, 201 <Quelle: https://pytorch.org/vision/main/models/densenet.html>.

\vspace{1cm}

<Figure xy dense block -> figure 1 in Densenet paper>

\subsection{MobileNetV3}

The goal of the MobileNet series is to efficiently operate on limited hardware for example on mobile devices.
Therefore the main focus of these models is to be as lightweight as possible <Quelle: Übersicht>, meaning the reduction of parameters and ensuring real-time capabilities while maintaining comparable accuracy.
There are 3 versions of MobileNets and each one builds upon and extends its predecessor.
To achieve a reduced size version 1 utilizes depthwise separable convolutions.
These blocks of convolutions consist of two steps. First, a depthwise convolution, where a kernel is deployed on each channel, followed by a pointwise convolution, where a 1x1 kernel combines the output from the first step <Quelle: Übersicht>.
MobileNet V2 then added a layer with a 1x1 kernel before the depthwise convolution and a residual connection to create its so-called bottleneck blocks.
A bottleneck block from MobileNetV2 is shown in <Figure xy>.

\vspace{1cm}
<Figure 3 MobileNetV3>
\vspace{1cm}

MobileNet V3 added optional blocks called Squeeze-and-Excite, which is illustrated in <Figure xy>. 
These blocks can be added to any CNN and consist of a global pooling layer and two fully connected layers with a ReLu and a Sigmoid activation function respectively.
After that, the output of this additional block is multiplied with the feature map.
This enables the weighing of specific channels <Quelle: https://arxiv.org/abs/1709.01507>.
Additionally, for the MobileNet V3 a Network Search is used to find optimal network structures and experiments with various activation functions have been conducted <Quelle: Mobilenet V3>.

\vspace{1cm}
<Figure 4 MobileNetV3>
\vspace{1cm}

MobileNets allow flexible usage with the so-called width or in V3 depth multiplier, which is a hyperparameter for controlling the number of feature maps in layers.
MobileNet V1 and V2 additionally offer the resolution multiplier, which controls the resolution of layers.
Both the MobileNetV3\_small and MobileNetV3\_large are supported in PyTorch <Quelle: https://pytorch.org/vision/main/models/mobilenetv3.html>.

\subsection{EfficientNets}

blindtext


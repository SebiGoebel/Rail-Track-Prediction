\section{Network architectures}
\label{sec:networkArchitectures}

One of the first CNN architectures was the Le-Net5 <Quelle>.
This CNN was proposed to classify handwritten digits.
It used 5 convolution and pooling layers.
14 years later CNNs were rediscovered with the introduction of AlexNet <Quelle: AlexNet> in 2012.
In the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <Quelle> in 2012 AlexNet outperformed traditional algorithms and showed the potential of CNNs.
Consequently, it is often considered the starting point of the CNN era in the literature.
Since then research has shown a rapid development of CNN architectures, consistently striving for greater accuracy or speed <Quelle: Übersicht>.

\vspace{1cm} % Größerer Abstand zwischen den Reihen

\noindent Several major advancements up to now include:

\begin{itemize}
    \item In 2014 two popular architectures were introduced, which still are often used for state-of-the-art comparisons. Firstly, the VGGNet <Quelle> used small three-by-three filters and many layers providing a deep network architecture. Secondly, the Inception network or GoogLeNet <Quelle>. To extract features in a range of various scales, this architecture utilized different filter sizes.
    \item In 2015 the ResNet architecture <Quelle: ResNet> introduced the concept of residual learning with shortcut connections. By skipping layers, this framework enabled the training of deeper networks with over a hundred layers.
    \item In 2016 DenseNets <Quelle> was introduced to further deal with the problem of vanishing gradients. This architecture implemented a connection between each layer and every following layer ensuring maximum information flow.
    \item In 2017 MobileNets <Quelle> was developed for applications in mobile devices. Since hardware with limited capabilities is used, the main focus of MobileNet architectures lies on efficiency. The utilization of depthwise separable convolutions allowed small model sizes and fast latencies.
    \item In 2019 EfficientNets <Quelle> was introduced to further explore greater trade-offs between accuracy and efficiency, by using scaling methods for model architectures. 
\end{itemize}

\subsection{Example Überschrift}

blindtext

\subsection{Example Überschrift}

blindtext hallo
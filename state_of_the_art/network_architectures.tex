\section{Network architectures}
\label{sec:networkArchitectures}

One of the first CNN architectures was the \textbf{Le-Net5} \cite{LeNet5}.
This CNN was proposed to classify handwritten digits.
It used 5 convolution and pooling layers.
14 years later CNNs were rediscovered with the introduction of \textbf{AlexNet} \cite{AlexNet2012} in 2012.
In the \ac{ILSVRC} \cite{ILSVRC15} in 2012 \textbf{AlexNet} outperformed traditional algorithms and showed the potential of CNNs.
Consequently, it is often considered the starting point of the CNN era in the literature.
Since then research has shown a rapid development of CNN architectures, consistently striving for greater accuracy or speed \cite{networkArchitectureSurvey}.

\vspace{1cm} % Größerer Abstand zwischen den Reihen

\noindent Several major advancements up to now include:

\begin{itemize}
    \item In 2014 two popular architectures were introduced, which still are often used for state-of-the-art comparisons. Firstly, the \textbf{\ac{VGGNet}} \cite{VGGNet2015} used small three-by-three filters and many layers providing a deep network architecture. Secondly, the \textbf{Inception} network or \textbf{GoogLeNet} \cite{InceptionNet}. To extract features in a range of various scales, this architecture utilized different filter sizes.
    \item In 2015 the \textbf{ResNet} architecture \cite{ResNet} introduced the concept of residual learning with shortcut connections. By skipping layers, this framework enabled the training of deeper networks with over a hundred layers.
    \item In 2016 \textbf{DenseNets} \cite{DenseNets} was introduced to further deal with the problem of vanishing gradients. This architecture implemented a connection between each layer and every following layer ensuring maximum information flow.
    \item In 2017 \textbf{MobileNets} \cite{MobileNetV1} \cite{MobileNetV2} \cite{MobileNetV3} was developed for applications in mobile devices. Since hardware with limited capabilities is used, the main focus of MobileNet architectures lies on efficiency. The utilization of depthwise separable convolutions allowed small model sizes and fast latencies.
    \item In 2019 \textbf{EfficientNets} \cite{EfficientNet} was introduced to further explore greater trade-offs between accuracy and efficiency, by using scaling methods for model architectures. 
\end{itemize}

\vspace{1cm} % Größerer Abstand zwischen den Reihen

\noindent Since the CNNs are a part of a rapidly developing research field, more recent architectures are more interesting for this work.
Because they already solved issues that exist in older models.
Therefore, only the last four mentioned CNNs are described in more detail in the next sections.

\subsection{ResNets}

https://medium.com/@cpt1995daas/evolution-of-convolutional-neural-network-cnn-architectures-44f2109268a1

A widely used model architecture in the community is ResNet <Quelle: ResNet>.
Up to this architecture CNNs suffered from the vanishing gradient problem, particularly when the model consists of a deep network.
A neural network is typically considered deep when it comprises a large number of consecutive layers.
To solve that issue ResNet introduced the residual block that is also visualized in <Figure xy>.
This block enabled effective training of models with up to 152 layers. Compared to VGGNet ResNet is 8 times deeper but still less complex.
ResNet showed a great performance gain and was therefore granted first place in the classification task of the ILSVRC 2015.
ResNet is supported in Pytorch, which includes variants with 18, 34, 50, 101, and 152 layers <Quelle: https://pytorch.org/vision/main/models/resnet.html>. 
However, ResNet models are still resource-intensive because of their size <Quelle: Übersicht>. 

\vspace{1cm}

<Figure xy> <Quelle: ResNet>

\vspace{1cm}

<Figure xy> schematically represents the ResNet's skip connection.
The block has two processes.
One consists of two layers and the other one performs identity mapping.
After that, the outputs are added together.

\subsection{DenseNets}

DenseNet further explores skip connections and implemented architecture blocks in which each layer is connected to every following layer.
An example of a so-called "dense block" is shown in <Figure xy>.
Compared to ResNet, this model connects outputs via concatenation instead of addition.
Between dense bocks, there are the so-called "transition layers", consisting of one convolutional and then one pooling layer.
This architecture allows deeper CNNs which result in more accuracy and efficiency.
DenseNet showed great results with the introduced technique because their blocks further tackle the vanishing gradient problem with feature reuse.
PyTorch includes DenseNet model variants with 121, 161, 169, 201 <Quelle: https://pytorch.org/vision/main/models/densenet.html>.

\vspace{1cm}

<Figure xy dense block -> figure 1 in Densenet paper>

\subsection{MobileNetV3}

blindtext

\subsection{EfficientNets}

blindtext


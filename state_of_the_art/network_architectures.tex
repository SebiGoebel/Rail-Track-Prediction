\section{Network architectures}
\label{sec:networkArchitectures}

One of the first CNN architectures was the \textbf{Le-Net5} \cite{LeNet5}.
This CNN was proposed to classify handwritten digits.
It used 5 convolution and pooling layers.
14 years later CNNs were rediscovered with the introduction of \textbf{AlexNet} <Quelle: AlexNet> in 2012.
In the \ac{ILSVRC} <Quelle> in 2012 \textbf{AlexNet} outperformed traditional algorithms and showed the potential of CNNs.
Consequently, it is often considered the starting point of the CNN era in the literature.
Since then research has shown a rapid development of CNN architectures, consistently striving for greater accuracy or speed \cite{networkArchitectureSurvey}.

\vspace{1cm} % Größerer Abstand zwischen den Reihen

\noindent Several major advancements up to now include:

\begin{itemize}
    \item In 2014 two popular architectures were introduced, which still are often used for state-of-the-art comparisons. Firstly, the \textbf{\ac{VGGNet}} \cite{VGGNet2015} used small three-by-three filters and many layers providing a deep network architecture. Secondly, the \textbf{Inception} network or \textbf{GoogLeNet} \cite{InceptionNet}. To extract features in a range of various scales, this architecture utilized different filter sizes.
    \item In 2015 the \textbf{ResNet} architecture \cite{ResNet} introduced the concept of residual learning with shortcut connections. By skipping layers, this framework enabled the training of deeper networks with over a hundred layers.
    \item In 2016 \textbf{DenseNets} \cite{DenseNets} was introduced to further deal with the problem of vanishing gradients. This architecture implemented a connection between each layer and every following layer ensuring maximum information flow.
    \item In 2017 \textbf{MobileNets} \cite{MobileNetV1} \cite{MobileNetV2} \cite{MobileNetV2Arxiv} \cite{MobileNetV3} was developed for applications in mobile devices. Since hardware with limited capabilities is used, the main focus of MobileNet architectures lies on efficiency. The utilization of depthwise separable convolutions allowed small model sizes and fast latencies.
    \item In 2019 \textbf{EfficientNets} \cite{EfficientNet} was introduced to further explore greater trade-offs between accuracy and efficiency, by using scaling methods for model architectures. 
\end{itemize}

\subsection{ResNets}

https://medium.com/@cpt1995daas/evolution-of-convolutional-neural-network-cnn-architectures-44f2109268a1

blindtext

\subsection{DenseNets}

blindtext

\subsection{MobileNetV3}

blindtext

\subsection{EfficientNets}

blindtext

